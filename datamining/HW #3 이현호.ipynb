{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FATLIVER</th>\n",
       "      <th>FAT1</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>DRINK</th>\n",
       "      <th>SMOKING</th>\n",
       "      <th>SYSBP</th>\n",
       "      <th>DIABP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>HB</th>\n",
       "      <th>GOT</th>\n",
       "      <th>GPT</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>GLUCOSE</th>\n",
       "      <th>TG</th>\n",
       "      <th>HDLC</th>\n",
       "      <th>LDLC</th>\n",
       "      <th>ALB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130</td>\n",
       "      <td>82</td>\n",
       "      <td>101.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "      <td>116</td>\n",
       "      <td>38</td>\n",
       "      <td>35.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>75</td>\n",
       "      <td>120.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>224</td>\n",
       "      <td>96</td>\n",
       "      <td>46</td>\n",
       "      <td>57.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156</td>\n",
       "      <td>96</td>\n",
       "      <td>102.6</td>\n",
       "      <td>13.3</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>33.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133</td>\n",
       "      <td>81</td>\n",
       "      <td>90.3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>198</td>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>56.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173</td>\n",
       "      <td>99</td>\n",
       "      <td>151.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>141</td>\n",
       "      <td>155</td>\n",
       "      <td>179</td>\n",
       "      <td>31.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>507</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>127</td>\n",
       "      <td>71</td>\n",
       "      <td>107.2</td>\n",
       "      <td>14.7</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>216</td>\n",
       "      <td>99</td>\n",
       "      <td>151</td>\n",
       "      <td>54.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>508</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>107.3</td>\n",
       "      <td>14.6</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>88</td>\n",
       "      <td>65</td>\n",
       "      <td>55.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>509</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>149</td>\n",
       "      <td>87</td>\n",
       "      <td>113.5</td>\n",
       "      <td>16.3</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>274</td>\n",
       "      <td>228</td>\n",
       "      <td>22</td>\n",
       "      <td>44.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>510</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135</td>\n",
       "      <td>81</td>\n",
       "      <td>114.4</td>\n",
       "      <td>13.8</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>180</td>\n",
       "      <td>107</td>\n",
       "      <td>162</td>\n",
       "      <td>47.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>54</td>\n",
       "      <td>109.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "      <td>82</td>\n",
       "      <td>42</td>\n",
       "      <td>47.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  FATLIVER  FAT1  AGE  GENDER  DRINK  SMOKING  SYSBP  DIABP    BMI  \\\n",
       "0      1         1     1   40       1    2.0      1.0    130     82  101.1   \n",
       "1      2         0     0   51       2    1.0      1.0    114     75  120.7   \n",
       "2      3         0     0   55       2    1.0      1.0    156     96  102.6   \n",
       "3      4         0     0   30       1    2.0      3.0    133     81   90.3   \n",
       "4      5         0     0   61       2    1.0      1.0    173     99  151.0   \n",
       "..   ...       ...   ...  ...     ...    ...      ...    ...    ...    ...   \n",
       "506  507         2     1   51       1    3.0      2.0    127     71  107.2   \n",
       "507  508         0     0   49       1    2.0      1.0    130     76  107.3   \n",
       "508  509         2     1   51       1    2.0      3.0    149     87  113.5   \n",
       "509  510         0     0   46       2    1.0      1.0    135     81  114.4   \n",
       "510  511         1     1   38       2    2.0      1.0    101     54  109.7   \n",
       "\n",
       "       HB  GOT  GPT  TCHOL  GLUCOSE   TG  HDLC   LDLC  ALB  \n",
       "0    12.5   20   19    148      116   38  35.0  106.0  3.9  \n",
       "1    13.1   15   11    224       96   46  57.0  158.0  4.0  \n",
       "2    13.3   16   12    242       88   58  33.0  197.0  4.1  \n",
       "3    14.8   12    8    198       79   70  56.0  128.0  4.2  \n",
       "4    13.2   42   19    141      155  179  31.0   74.0  3.7  \n",
       "..    ...  ...  ...    ...      ...  ...   ...    ...  ...  \n",
       "506  14.7   19   21    216       99  151  54.0  132.0  4.3  \n",
       "507  14.6   15   12    164       88   65  55.0   96.0  4.2  \n",
       "508  16.3   18   25    274      228   22  44.0  186.0  4.1  \n",
       "509  13.8   19   12    180      107  162  47.0  120.0  4.2  \n",
       "510  12.7   20   13    170       82   42  47.0  114.0  3.9  \n",
       "\n",
       "[511 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('지방간데이터.xls') # 분석할 데이터 불러오기\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'FATLIVER', 'FAT1', 'AGE', 'GENDER', 'DRINK', 'SMOKING', 'SYSBP',\n",
       "       'DIABP', 'BMI', 'HB', 'GOT', 'GPT', 'TCHOL', 'GLUCOSE', 'TG', 'HDLC',\n",
       "       'LDLC', 'ALB'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FAT1</th>\n",
       "      <th>AGE</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>DRINK</th>\n",
       "      <th>SMOKING</th>\n",
       "      <th>SYSBP</th>\n",
       "      <th>DIABP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>HB</th>\n",
       "      <th>GOT</th>\n",
       "      <th>GPT</th>\n",
       "      <th>TCHOL</th>\n",
       "      <th>GLUCOSE</th>\n",
       "      <th>TG</th>\n",
       "      <th>HDLC</th>\n",
       "      <th>LDLC</th>\n",
       "      <th>ALB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130</td>\n",
       "      <td>82</td>\n",
       "      <td>101.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "      <td>116</td>\n",
       "      <td>38</td>\n",
       "      <td>35.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114</td>\n",
       "      <td>75</td>\n",
       "      <td>120.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>224</td>\n",
       "      <td>96</td>\n",
       "      <td>46</td>\n",
       "      <td>57.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156</td>\n",
       "      <td>96</td>\n",
       "      <td>102.6</td>\n",
       "      <td>13.3</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>242</td>\n",
       "      <td>88</td>\n",
       "      <td>58</td>\n",
       "      <td>33.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133</td>\n",
       "      <td>81</td>\n",
       "      <td>90.3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>198</td>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>56.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173</td>\n",
       "      <td>99</td>\n",
       "      <td>151.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>141</td>\n",
       "      <td>155</td>\n",
       "      <td>179</td>\n",
       "      <td>31.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>127</td>\n",
       "      <td>71</td>\n",
       "      <td>107.2</td>\n",
       "      <td>14.7</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>216</td>\n",
       "      <td>99</td>\n",
       "      <td>151</td>\n",
       "      <td>54.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130</td>\n",
       "      <td>76</td>\n",
       "      <td>107.3</td>\n",
       "      <td>14.6</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>164</td>\n",
       "      <td>88</td>\n",
       "      <td>65</td>\n",
       "      <td>55.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>149</td>\n",
       "      <td>87</td>\n",
       "      <td>113.5</td>\n",
       "      <td>16.3</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>274</td>\n",
       "      <td>228</td>\n",
       "      <td>22</td>\n",
       "      <td>44.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135</td>\n",
       "      <td>81</td>\n",
       "      <td>114.4</td>\n",
       "      <td>13.8</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>180</td>\n",
       "      <td>107</td>\n",
       "      <td>162</td>\n",
       "      <td>47.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>54</td>\n",
       "      <td>109.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "      <td>82</td>\n",
       "      <td>42</td>\n",
       "      <td>47.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FAT1  AGE  GENDER  DRINK  SMOKING  SYSBP  DIABP    BMI    HB  GOT  GPT  \\\n",
       "0       1   40       1    2.0      1.0    130     82  101.1  12.5   20   19   \n",
       "1       0   51       2    1.0      1.0    114     75  120.7  13.1   15   11   \n",
       "2       0   55       2    1.0      1.0    156     96  102.6  13.3   16   12   \n",
       "3       0   30       1    2.0      3.0    133     81   90.3  14.8   12    8   \n",
       "4       0   61       2    1.0      1.0    173     99  151.0  13.2   42   19   \n",
       "..    ...  ...     ...    ...      ...    ...    ...    ...   ...  ...  ...   \n",
       "506     1   51       1    3.0      2.0    127     71  107.2  14.7   19   21   \n",
       "507     0   49       1    2.0      1.0    130     76  107.3  14.6   15   12   \n",
       "508     1   51       1    2.0      3.0    149     87  113.5  16.3   18   25   \n",
       "509     0   46       2    1.0      1.0    135     81  114.4  13.8   19   12   \n",
       "510     1   38       2    2.0      1.0    101     54  109.7  12.7   20   13   \n",
       "\n",
       "     TCHOL  GLUCOSE   TG  HDLC   LDLC  ALB  \n",
       "0      148      116   38  35.0  106.0  3.9  \n",
       "1      224       96   46  57.0  158.0  4.0  \n",
       "2      242       88   58  33.0  197.0  4.1  \n",
       "3      198       79   70  56.0  128.0  4.2  \n",
       "4      141      155  179  31.0   74.0  3.7  \n",
       "..     ...      ...  ...   ...    ...  ...  \n",
       "506    216       99  151  54.0  132.0  4.3  \n",
       "507    164       88   65  55.0   96.0  4.2  \n",
       "508    274      228   22  44.0  186.0  4.1  \n",
       "509    180      107  162  47.0  120.0  4.2  \n",
       "510    170       82   42  47.0  114.0  3.9  \n",
       "\n",
       "[511 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df.drop(['ID','FATLIVER'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FAT1        0\n",
       "AGE         0\n",
       "GENDER      0\n",
       "DRINK       1\n",
       "SMOKING     1\n",
       "SYSBP       0\n",
       "DIABP       0\n",
       "BMI         0\n",
       "HB          0\n",
       "GOT         0\n",
       "GPT         0\n",
       "TCHOL       0\n",
       "GLUCOSE     0\n",
       "TG          0\n",
       "HDLC       11\n",
       "LDLC       11\n",
       "ALB         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 개수\n",
    "data.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n",
      "[[  0   0]\n",
      " [  0   1]\n",
      " [  0   2]\n",
      " ...\n",
      " [510  14]\n",
      " [510  15]\n",
      " [510  16]]\n",
      "(8662, 2)\n"
     ]
    }
   ],
   "source": [
    "notna_bool = data.notna()\n",
    "notna_bool_arr = notna_bool.to_numpy()\n",
    "print(notna_bool_arr)\n",
    "non_missing_idx = np.argwhere(data.notna().to_numpy())\n",
    "print(non_missing_idx)\n",
    "print(non_missing_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 아닌 위치 array를 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "idx = np.arange(non_missing_idx.shape[0])\n",
    "# option 1. absolute size of test set\n",
    "num_test = 1000\n",
    "train_idx, test_idx = train_test_split(idx, test_size=num_test)\n",
    "# option 2. ratio of test set\n",
    "ratio_test = 0.2\n",
    "train_idx, test_idx = train_test_split(idx, test_size=ratio_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105 120 382 ... 492 313  64] [ 4 12  2 ... 14  3  9]\n"
     ]
    }
   ],
   "source": [
    "mask_row = non_missing_idx[test_idx, 0] # 첫 번째 열 받아옴\n",
    "mask_col = non_missing_idx[test_idx, 1] # 두 번째 열 받아옴\n",
    "print(mask_row, mask_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1. ,  40. ,   1. , ...,  35. , 106. ,   3.9],\n",
       "       [  0. ,  51. ,   2. , ...,  57. , 158. ,   4. ],\n",
       "       [  0. ,  55. ,   2. , ...,  33. , 197. ,   4.1],\n",
       "       ...,\n",
       "       [  1. ,  51. ,   1. , ...,  44. , 186. ,   4.1],\n",
       "       [  0. ,  46. ,   2. , ...,  47. , 120. ,   4.2],\n",
       "       [  1. ,  38. ,   2. , ...,  47. , 114. ,   3.9]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset 복사 후 test_set 제거\n",
    "data_incomplete = np.array(data.copy()) # deep copy\n",
    "data_incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original\n",
      "    FAT1  AGE  GENDER  DRINK  SMOKING  SYSBP  DIABP    BMI    HB  GOT  GPT  \\\n",
      "0      1   40       1    2.0      1.0    130     82  101.1  12.5   20   19   \n",
      "1      0   51       2    1.0      1.0    114     75  120.7  13.1   15   11   \n",
      "2      0   55       2    1.0      1.0    156     96  102.6  13.3   16   12   \n",
      "3      0   30       1    2.0      3.0    133     81   90.3  14.8   12    8   \n",
      "4      0   61       2    1.0      1.0    173     99  151.0  13.2   42   19   \n",
      "5      0   47       2    1.0      1.0    117     61  100.2  13.3   19   15   \n",
      "6      1   25       1    1.0      1.0    124     74  130.7  15.7   30   40   \n",
      "7      1   36       1    2.0      1.0    111     65   95.5  14.9   16   20   \n",
      "8      0   32       1    1.0      3.0    118     72   99.9  16.2   23   15   \n",
      "9      0   30       2    1.0      1.0    110     71  109.7  12.9   21    9   \n",
      "10     0   59       1    2.0      3.0    129     80   93.7  14.0   16   11   \n",
      "11     0   51       1    1.0      1.0    122     76  113.4  15.0   23   23   \n",
      "12     1   36       1    1.0      1.0    101     60  108.5  15.3   17   11   \n",
      "13     0   31       1    2.0      1.0    103     58   94.2  15.1   20    8   \n",
      "14     0   26       2    1.0      1.0    165    100  108.5  12.7   14    7   \n",
      "15     0   52       2    1.0      1.0    129     77   90.2  13.6   24   21   \n",
      "16     1   60       1    1.0      1.0    141     84  146.3  16.3   50   38   \n",
      "17     1   31       1    2.0      3.0    130     81  116.4  17.3   30   56   \n",
      "18     0   58       1    1.0      3.0    108     75  107.0  15.2   18   14   \n",
      "19     1   38       1    2.0      1.0    114     73  155.7  14.8   28   44   \n",
      "\n",
      "    TCHOL  GLUCOSE   TG  HDLC   LDLC  ALB  \n",
      "0     148      116   38  35.0  106.0  3.9  \n",
      "1     224       96   46  57.0  158.0  4.0  \n",
      "2     242       88   58  33.0  197.0  4.1  \n",
      "3     198       79   70  56.0  128.0  4.2  \n",
      "4     141      155  179  31.0   74.0  3.7  \n",
      "5     199       85   68  42.0  144.0  4.1  \n",
      "6     214       97   66  43.0  157.0  4.3  \n",
      "7     167       97   78  33.0  120.0  4.1  \n",
      "8     168       85   40  30.0  129.0  4.5  \n",
      "9     135       94   72  32.0   89.0  4.3  \n",
      "10    235       95  141  35.0  172.0  3.9  \n",
      "11    141       97  194  23.0   79.0  4.4  \n",
      "12    164       90   66  32.0  119.0  4.4  \n",
      "13    150       87   55   NaN    NaN  4.2  \n",
      "14    200       96   59   NaN    NaN  4.5  \n",
      "15    136       84  140  35.0   73.0  3.9  \n",
      "16    213       98  151  29.0  154.0  4.3  \n",
      "17    213       94  188  32.0  144.0  4.6  \n",
      "18    191      121  108  36.0  134.0  3.9  \n",
      "19    217       98   79  31.0  169.0  4.1  \n",
      "\n",
      "Masked\n",
      "      0     1    2    3    4      5      6      7     8     9    10     11  \\\n",
      "0   NaN  40.0  1.0  2.0  NaN  130.0   82.0  101.1   NaN  20.0   NaN  148.0   \n",
      "1   NaN  51.0  2.0  1.0  1.0  114.0   75.0    NaN  13.1  15.0   NaN  224.0   \n",
      "2   0.0  55.0  2.0  1.0  1.0  156.0   96.0  102.6   NaN  16.0  12.0  242.0   \n",
      "3   0.0  30.0  1.0  2.0  3.0  133.0   81.0   90.3  14.8  12.0   8.0    NaN   \n",
      "4   0.0  61.0  2.0  1.0  NaN  173.0   99.0  151.0  13.2  42.0  19.0  141.0   \n",
      "5   0.0   NaN  NaN  1.0  1.0  117.0   61.0    NaN   NaN   NaN  15.0  199.0   \n",
      "6   1.0  25.0  1.0  1.0  1.0  124.0   74.0    NaN  15.7  30.0   NaN    NaN   \n",
      "7   NaN  36.0  NaN  2.0  1.0    NaN   65.0   95.5  14.9  16.0   NaN    NaN   \n",
      "8   NaN  32.0  1.0  1.0  3.0  118.0   72.0    NaN  16.2  23.0  15.0    NaN   \n",
      "9   0.0  30.0  2.0  NaN  1.0  110.0    NaN  109.7  12.9  21.0   9.0  135.0   \n",
      "10  0.0  59.0  1.0  2.0  3.0  129.0   80.0   93.7  14.0  16.0  11.0  235.0   \n",
      "11  0.0  51.0  1.0  1.0  1.0  122.0   76.0  113.4  15.0  23.0  23.0  141.0   \n",
      "12  NaN   NaN  NaN  NaN  1.0  101.0   60.0  108.5   NaN  17.0  11.0    NaN   \n",
      "13  NaN   NaN  1.0  2.0  1.0  103.0   58.0   94.2  15.1  20.0   8.0    NaN   \n",
      "14  0.0   NaN  2.0  1.0  1.0  165.0  100.0  108.5  12.7  14.0   NaN  200.0   \n",
      "15  0.0  52.0  NaN  1.0  1.0  129.0    NaN   90.2  13.6   NaN  21.0  136.0   \n",
      "16  NaN  60.0  1.0  1.0  1.0    NaN   84.0  146.3   NaN   NaN  38.0  213.0   \n",
      "17  1.0  31.0  1.0  2.0  3.0  130.0   81.0  116.4  17.3  30.0  56.0  213.0   \n",
      "18  0.0  58.0  1.0  1.0  3.0  108.0   75.0  107.0  15.2  18.0  14.0    NaN   \n",
      "19  1.0  38.0  1.0  2.0  NaN  114.0    NaN  155.7  14.8  28.0   NaN  217.0   \n",
      "\n",
      "       12     13    14     15   16  \n",
      "0   116.0   38.0  35.0    NaN  3.9  \n",
      "1    96.0   46.0   NaN    NaN  4.0  \n",
      "2     NaN    NaN  33.0  197.0  4.1  \n",
      "3    79.0   70.0  56.0  128.0  NaN  \n",
      "4   155.0  179.0  31.0   74.0  3.7  \n",
      "5    85.0   68.0  42.0  144.0  4.1  \n",
      "6    97.0   66.0  43.0  157.0  NaN  \n",
      "7     NaN    NaN  33.0    NaN  4.1  \n",
      "8    85.0    NaN  30.0  129.0  4.5  \n",
      "9    94.0   72.0  32.0   89.0  NaN  \n",
      "10   95.0  141.0   NaN  172.0  3.9  \n",
      "11   97.0  194.0  23.0   79.0  NaN  \n",
      "12   90.0   66.0  32.0    NaN  4.4  \n",
      "13   87.0   55.0   NaN    NaN  NaN  \n",
      "14   96.0   59.0   NaN    NaN  4.5  \n",
      "15    NaN  140.0  35.0   73.0  3.9  \n",
      "16   98.0  151.0  29.0  154.0  NaN  \n",
      "17   94.0  188.0  32.0  144.0  4.6  \n",
      "18  121.0  108.0   NaN  134.0  3.9  \n",
      "19   98.0   79.0  31.0  169.0  4.1  \n"
     ]
    }
   ],
   "source": [
    "data_incomplete[mask_row, mask_col] = np.nan\n",
    "\n",
    "print('Original')\n",
    "print(data.head(20))\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Masked')\n",
    "print(pd.DataFrame(data_incomplete).head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='most_frequent', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1    2    3    4      5      6      7     8     9    10     11  \\\n",
      "0   1.0  40.0  1.0  2.0  1.0  130.0   82.0  101.1  12.5  20.0  19.0  148.0   \n",
      "1   0.0  51.0  2.0  1.0  1.0  114.0   75.0  120.7  13.1  15.0  11.0  224.0   \n",
      "2   0.0  55.0  2.0  1.0  1.0  156.0   96.0  102.6  13.3  16.0  12.0  242.0   \n",
      "3   0.0  30.0  1.0  2.0  3.0  133.0   81.0   90.3  14.8  12.0   8.0  198.0   \n",
      "4   0.0  61.0  2.0  1.0  1.0  173.0   99.0  151.0  13.2  42.0  19.0  141.0   \n",
      "5   0.0  47.0  2.0  1.0  1.0  117.0   61.0  100.2  13.3  19.0  15.0  199.0   \n",
      "6   1.0  25.0  1.0  1.0  1.0  124.0   74.0  130.7  15.7  30.0  40.0  214.0   \n",
      "7   1.0  36.0  1.0  2.0  1.0  111.0   65.0   95.5  14.9  16.0  20.0  167.0   \n",
      "8   0.0  32.0  1.0  1.0  3.0  118.0   72.0   99.9  16.2  23.0  15.0  168.0   \n",
      "9   0.0  30.0  2.0  1.0  1.0  110.0   71.0  109.7  12.9  21.0   9.0  135.0   \n",
      "10  0.0  59.0  1.0  2.0  3.0  129.0   80.0   93.7  14.0  16.0  11.0  235.0   \n",
      "11  0.0  51.0  1.0  1.0  1.0  122.0   76.0  113.4  15.0  23.0  23.0  141.0   \n",
      "12  1.0  36.0  1.0  1.0  1.0  101.0   60.0  108.5  15.3  17.0  11.0  164.0   \n",
      "13  0.0  31.0  1.0  2.0  1.0  103.0   58.0   94.2  15.1  20.0   8.0  150.0   \n",
      "14  0.0  26.0  2.0  1.0  1.0  165.0  100.0  108.5  12.7  14.0   7.0  200.0   \n",
      "15  0.0  52.0  2.0  1.0  1.0  129.0   77.0   90.2  13.6  24.0  21.0  136.0   \n",
      "16  1.0  60.0  1.0  1.0  1.0  141.0   84.0  146.3  16.3  50.0  38.0  213.0   \n",
      "17  1.0  31.0  1.0  2.0  3.0  130.0   81.0  116.4  17.3  30.0  56.0  213.0   \n",
      "18  0.0  58.0  1.0  1.0  3.0  108.0   75.0  107.0  15.2  18.0  14.0  191.0   \n",
      "19  1.0  38.0  1.0  2.0  1.0  114.0   73.0  155.7  14.8  28.0  44.0  217.0   \n",
      "\n",
      "       12     13    14     15   16  \n",
      "0   116.0   38.0  35.0  106.0  3.9  \n",
      "1    96.0   46.0  57.0  158.0  4.0  \n",
      "2    88.0   58.0  33.0  197.0  4.1  \n",
      "3    79.0   70.0  56.0  128.0  4.2  \n",
      "4   155.0  179.0  31.0   74.0  3.7  \n",
      "5    85.0   68.0  42.0  144.0  4.1  \n",
      "6    97.0   66.0  43.0  157.0  4.3  \n",
      "7    97.0   78.0  33.0  120.0  4.1  \n",
      "8    85.0   40.0  30.0  129.0  4.5  \n",
      "9    94.0   72.0  32.0   89.0  4.3  \n",
      "10   95.0  141.0  35.0  172.0  3.9  \n",
      "11   97.0  194.0  23.0   79.0  4.4  \n",
      "12   90.0   66.0  32.0  119.0  4.4  \n",
      "13   87.0   55.0  33.0  119.0  4.2  \n",
      "14   96.0   59.0  33.0  119.0  4.5  \n",
      "15   84.0  140.0  35.0   73.0  3.9  \n",
      "16   98.0  151.0  29.0  154.0  4.3  \n",
      "17   94.0  188.0  32.0  144.0  4.6  \n",
      "18  121.0  108.0  36.0  134.0  3.9  \n",
      "19   98.0   79.0  31.0  169.0  4.1  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "imputer_mode = SimpleImputer(missing_values=np.nan, strategy ='most_frequent')\n",
    "imputer_mode.fit(data)\n",
    "data_filled_mode = imputer_mode.transform(data)\n",
    "dfm = pd.DataFrame(data_filled_mode)\n",
    "print(dfm.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>101.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>120.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>102.6</td>\n",
       "      <td>13.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>107.2</td>\n",
       "      <td>14.7</td>\n",
       "      <td>19.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>107.3</td>\n",
       "      <td>14.6</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>113.5</td>\n",
       "      <td>16.3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>46.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>114.4</td>\n",
       "      <td>13.8</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>109.7</td>\n",
       "      <td>12.7</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        1    2    3    4      5     6      7     8     9    10     11     12  \\\n",
       "0    40.0  1.0  2.0  1.0  130.0  82.0  101.1  12.5  20.0  19.0  148.0  116.0   \n",
       "1    51.0  2.0  1.0  1.0  114.0  75.0  120.7  13.1  15.0  11.0  224.0   96.0   \n",
       "2    55.0  2.0  1.0  1.0  156.0  96.0  102.6  13.3  16.0  12.0  242.0   88.0   \n",
       "3    30.0  1.0  2.0  3.0  133.0  81.0   90.3  14.8  12.0   8.0  198.0   79.0   \n",
       "4    61.0  2.0  1.0  1.0  173.0  99.0  151.0  13.2  42.0  19.0  141.0  155.0   \n",
       "..    ...  ...  ...  ...    ...   ...    ...   ...   ...   ...    ...    ...   \n",
       "506  51.0  1.0  3.0  2.0  127.0  71.0  107.2  14.7  19.0  21.0  216.0   99.0   \n",
       "507  49.0  1.0  2.0  1.0  130.0  76.0  107.3  14.6  15.0  12.0  164.0   88.0   \n",
       "508  51.0  1.0  2.0  3.0  149.0  87.0  113.5  16.3  18.0  25.0  274.0  228.0   \n",
       "509  46.0  2.0  1.0  1.0  135.0  81.0  114.4  13.8  19.0  12.0  180.0  107.0   \n",
       "510  38.0  2.0  2.0  1.0  101.0  54.0  109.7  12.7  20.0  13.0  170.0   82.0   \n",
       "\n",
       "        13    14     15   16  \n",
       "0     38.0  35.0  106.0  3.9  \n",
       "1     46.0  57.0  158.0  4.0  \n",
       "2     58.0  33.0  197.0  4.1  \n",
       "3     70.0  56.0  128.0  4.2  \n",
       "4    179.0  31.0   74.0  3.7  \n",
       "..     ...   ...    ...  ...  \n",
       "506  151.0  54.0  132.0  4.3  \n",
       "507   65.0  55.0   96.0  4.2  \n",
       "508   22.0  44.0  186.0  4.1  \n",
       "509  162.0  47.0  120.0  4.2  \n",
       "510   42.0  47.0  114.0  3.9  \n",
       "\n",
       "[511 rows x 16 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0      1.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "      ... \n",
       "506    1.0\n",
       "507    0.0\n",
       "508    1.0\n",
       "509    0.0\n",
       "510    1.0\n",
       "Name: 0, Length: 511, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dfm.iloc[:, 1:]\n",
    "y = dfm.iloc[:, 0]\n",
    "X\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pirl/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(287, 224)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2번째 독립변수(GENDER) 1 = 남, 2 = 여 \n",
    "male_idx = np.where(X.iloc[:, 1] == 1)[0]\n",
    "female_idx = np.where(X.iloc[:, 1] == 2)[0]\n",
    "\n",
    "X.iloc[male_idx, 1] = 'male'\n",
    "X.iloc[female_idx, 1] = 'female'\n",
    "\n",
    "sum(X.iloc[:, 1] == 'male'), sum(X.iloc[:, 1] == 'female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261, 152, 98)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3번째 독립변수(DRINK) 1 = 금주, 2 = 가끔, 3 = 자주\n",
    "never_idx = np.where(X.iloc[:, 2] == 1)[0]\n",
    "sometimes_idx = np.where(X.iloc[:, 2] == 2)[0]\n",
    "often_idx = np.where(X.iloc[:, 2] == 3)[0]\n",
    "X.iloc[never_idx, 2] = 'never'\n",
    "X.iloc[sometimes_idx, 2] = 'sometimes'\n",
    "X.iloc[often_idx, 2] = 'often'\n",
    "\n",
    "sum(X.iloc[:, 2] == 'never'), sum(X.iloc[:, 2] == 'sometimes'), sum(X.iloc[:, 2] == 'often')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(297, 51, 163)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4번째 독립변수(SMOKING) 1 = 금연, 2 = 과거흡연, 3 = 흡연\n",
    "never_idx = np.where(X.iloc[:, 3] == 1)[0]\n",
    "stop_idx = np.where(X.iloc[:, 3] == 2)[0]\n",
    "ing_idx = np.where(X.iloc[:, 3] == 3)[0]\n",
    "X.iloc[never_idx, 3] = 'never'\n",
    "X.iloc[stop_idx, 3] = 'stop'\n",
    "X.iloc[ing_idx, 3] = 'ing'\n",
    "\n",
    "sum(X.iloc[:, 3] == 'never'), sum(X.iloc[:, 3] == 'stop'), sum(X.iloc[:, 3] == 'ing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     float64\n",
       "2      object\n",
       "3      object\n",
       "4      object\n",
       "5     float64\n",
       "6     float64\n",
       "7     float64\n",
       "8     float64\n",
       "9     float64\n",
       "10    float64\n",
       "11    float64\n",
       "12    float64\n",
       "13    float64\n",
       "14    float64\n",
       "15    float64\n",
       "16    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>101.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>120.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>102.6</td>\n",
       "      <td>13.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>30.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>14.8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>61.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>13.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     1      5     6      7     8     9     10     11     12     13    14  \\\n",
       "0  40.0  130.0  82.0  101.1  12.5  20.0  19.0  148.0  116.0   38.0  35.0   \n",
       "1  51.0  114.0  75.0  120.7  13.1  15.0  11.0  224.0   96.0   46.0  57.0   \n",
       "2  55.0  156.0  96.0  102.6  13.3  16.0  12.0  242.0   88.0   58.0  33.0   \n",
       "3  30.0  133.0  81.0   90.3  14.8  12.0   8.0  198.0   79.0   70.0  56.0   \n",
       "4  61.0  173.0  99.0  151.0  13.2  42.0  19.0  141.0  155.0  179.0  31.0   \n",
       "\n",
       "      15   16  \n",
       "0  106.0  3.9  \n",
       "1  158.0  4.0  \n",
       "2  197.0  4.1  \n",
       "3  128.0  4.2  \n",
       "4   74.0  3.7  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>ing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>female</td>\n",
       "      <td>never</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        2          3      4\n",
       "0    male  sometimes  never\n",
       "1  female      never  never\n",
       "2  female      never  never\n",
       "3    male  sometimes    ing\n",
       "4  female      never  never"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_idx=np.where(X.dtypes=='float64')\n",
    "cat_idx=np.where(X.dtypes=='object')\n",
    "X_num = X.iloc[:, num_idx[0]]\n",
    "X_cat = X.iloc[:, cat_idx[0]]\n",
    "X_num.head()\n",
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = X_cat\n",
    "final_X = X_num\n",
    "\n",
    "for i in X_cat.columns:\n",
    "    cat_list = pd.get_dummies(tmp_df[i], drop_first = True, prefix = i)\n",
    "    final_X = final_X.join(cat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression 함수 import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit_reg = LogisticRegression()\n",
    "\n",
    "# Disciriminant 함수 import\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis() # 함수 명명\n",
    "qda = QuadraticDiscriminantAnalysis(store_covariance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-fold cross-validation 함수 import\n",
    "from sklearn.model_selection import KFold\n",
    "# scaling 함수 import\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "                           solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0, store_covariance=True,\n",
       "                              tol=0.0001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iter = 10 # 전체 실험 반복 횟수\n",
    "\n",
    "log_iter_train_result = [] # training set의 전체 값 초기화\n",
    "lda_iter_train_result = [] # training set의 전체 값 초기화\n",
    "qda_iter_train_result = [] # training set의 전체 값 초기화\n",
    "\n",
    "log_iter_test_result = [] # test set의 전체 값 초기화\n",
    "lda_iter_test_result = [] # test set의 전체 값 초기화\n",
    "qda_iter_test_result = [] # test set의 전체 값 초기화\n",
    "\n",
    "for epoch in np.arange(n_iter):\n",
    "    \n",
    "    cv = KFold(n_splits = 5, shuffle = True) # 5-fold cv 함수 입력\n",
    "\n",
    "    # training set의 5-fold R-squared 값 초기화\n",
    "    log_train_result = 0 \n",
    "    lda_train_result = 0\n",
    "    qda_train_result = 0\n",
    "\n",
    "    # test set의 5-fold R-squared 값 초기화\n",
    "    log_test_result = 0 \n",
    "    lda_test_result = 0\n",
    "    qda_test_result = 0\n",
    "    \n",
    "    for tr_idx, te_idx in cv.split(final_X):\n",
    "        \n",
    "        X_train = final_X.iloc[tr_idx] # cross-validation index를 통한 독립변수 training set\n",
    "        X_test = final_X.iloc[te_idx] # cross-validation index를 통한 독립변수 test set\n",
    "        y_train = y[tr_idx] # cross-validation index를 통한 종속변수 training set\n",
    "        y_test = y[te_idx] # cross-validation index를 통한 종속변수 test set\n",
    "        \n",
    "        scaler = StandardScaler() # scaler 함수 명명\n",
    "        scaler = scaler.fit(X_train.iloc[:, 0:np.shape(X_num)[1]]) # scaler 함수 fitting\n",
    "        # training set 내 연속형 데이터의 scaling\n",
    "        zs_X_train = scaler.transform(X_train.iloc[:, 0:np.shape(X_num)[1]])\n",
    "        # training set의 연속형과 이산형 데이터 합치기\n",
    "        zs_X_train = np.c_[zs_X_train, X_train.iloc[:, np.shape(X_num)[1]:].values]\n",
    "        \n",
    "        logit_reg.fit(zs_X_train, y_train) # Logistic regression 예측 모형 학습\n",
    "        lda.fit(zs_X_train, y_train) # LDA 학습\n",
    "        qda.fit(zs_X_train, y_train) # QDA 학습\n",
    "        \n",
    "        # test set 내 연속형 데이터의 scaling (training set의 정보 이용)\n",
    "        zs_X_test = scaler.transform(X_test.iloc[:, 0:np.shape(X_num)[1]])\n",
    "        # test set의 연속형과 이산형 데이터 합치기\n",
    "        zs_X_test = np.c_[zs_X_test, X_test.iloc[:, np.shape(X_num)[1]:].values]\n",
    "        \n",
    "        # 5-fold의 training set 학습 결과 저장\n",
    "        log_train_result += logit_reg.score(zs_X_train, y_train)\n",
    "        lda_train_result += lda.score(zs_X_train, y_train)\n",
    "        qda_train_result += qda.score(zs_X_train, y_train)\n",
    "        # 5-fold의 test set 학습 결과 저장\n",
    "        log_test_result += logit_reg.score(zs_X_test, y_test)\n",
    "        lda_test_result += lda.score(zs_X_test, y_test)\n",
    "        qda_test_result += qda.score(zs_X_test, y_test)\n",
    "        \n",
    "    # 5-fold의 training set 학습 결과의 평균\n",
    "    log_train_result = log_train_result / cv.get_n_splits(final_X)\n",
    "    lda_train_result = lda_train_result / cv.get_n_splits(final_X)\n",
    "    qda_train_result = qda_train_result / cv.get_n_splits(final_X)\n",
    "\n",
    "    # 5-fold의 test set 학습 결과의 평균\n",
    "    log_test_result = log_test_result / cv.get_n_splits(final_X)\n",
    "    lda_test_result = lda_test_result / cv.get_n_splits(final_X)\n",
    "    qda_test_result = qda_test_result / cv.get_n_splits(final_X)\n",
    "    \n",
    "    # training set의 전체 R-squared 값 저장\n",
    "    log_iter_train_result = log_train_result + [log_train_result]\n",
    "    lda_iter_train_result = lda_train_result  + [lda_train_result]\n",
    "    qda_iter_train_result = qda_train_result + [qda_train_result]\n",
    "    \n",
    "    # test set의 전체 R-squared 값 저장\n",
    "    log_iter_test_result = log_test_result + [log_test_result]\n",
    "    lda_iter_test_result = lda_test_result + [lda_test_result]\n",
    "    qda_iter_test_result = qda_test_result + [qda_test_result]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 모형 R-squared average :  1.682005369384918\n",
      "test R-squared average :  1.6244051018465637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean = np.mean(log_iter_train_result)\n",
    "test_mean = np.mean(log_iter_test_result)\n",
    "print(\"학습 모형 R-squared average : \",train_mean)\n",
    "print(\"test R-squared average : \",test_mean)\n",
    "\n",
    "test_std = np.std(log_iter_test_result)\n",
    "test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 모형 lda :  0.8449182607028141\n",
      "test lda :  0.8258899676375403\n",
      "학습 모형 qda :  0.8238781820796779\n",
      "test qda :  0.7848467542356748\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 모형 lda : \", np.mean(lda_train_result))\n",
    "print(\"test lda : \", np.mean(lda_test_result))\n",
    "\n",
    "print(\"학습 모형 qda : \", np.mean(qda_train_result))\n",
    "print(\"test qda : \", np.mean(qda_test_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측 모형의 종속변수 예측값\n",
    "logit_reg.predict(zs_X_train)\n",
    "lda.predict(zs_X_train)\n",
    "qda.predict(zs_X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
