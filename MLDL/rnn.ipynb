{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set torch constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Check\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "# dtype setting\n",
    "dtype = torch.double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show part of the data\n",
      "[[1.30528003e+03 1.31800000e+03 1.30436499e+03 1.25140000e+06\n",
      "  1.31137000e+03]\n",
      " [1.29428003e+03 1.32373999e+03 1.29424500e+03 2.03000000e+06\n",
      "  1.30885999e+03]\n",
      " [1.28945996e+03 1.29372998e+03 1.28250000e+03 1.15270000e+06\n",
      "  1.29180005e+03]]\n"
     ]
    }
   ],
   "source": [
    "stock_data = np.loadtxt(\"stock.csv\", delimiter=\",\")\n",
    "stock_data = stock_data[::-1] # Reverse the data\n",
    "\n",
    "print(\"Show part of the data\")\n",
    "print(stock_data[:3,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 10000\n",
    "\n",
    "seq_len = 2 # 얼마나 많은 T를사용할거지\n",
    "\n",
    "display_step = 1000 #\n",
    "\n",
    "D_in = 5 # Dimension of x (In this case, 5 feature per one day) \n",
    "\n",
    "H = 10\n",
    "D_out = 1\n",
    "Layer = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "[  1042.900024   1047.48999    1025.       710200.         1036.22998 ]\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data, test_ratio):\n",
    "    num_data = len(data)\n",
    "    print(num_data)\n",
    "    train_data = data[:int((1 - test_ratio) * num_data),:]\n",
    "    test_data = data[int((1 - test_ratio) * num_data):,:]\n",
    "    \n",
    "    return train_data, test_data\n",
    "train_data, test_data = split_dataset(stock_data, 0.2)\n",
    "min_data = np.min(train_data, 0)\n",
    "print(min_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    min_data = np.min(data, 0)\n",
    "    max_data = np.max(data, 0)\n",
    "    \n",
    "    numerator = data - min_data\n",
    "    denominator = max_data - min_data + 1e-8 # 1e-8 makes denominator not to be zero\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def construct_dataset(data, seq_len): #close 순서를 뒤로 배게 하기 위해서ㅏ 하는 \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    print(\"Construct the data...\")\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X_example = data[i:(i + seq_len), : ]\n",
    "        y_example = data[(i + seq_len), [-1]]\n",
    "        \n",
    "        X_data.append(X_example)\n",
    "        y_data.append(y_example)\n",
    "    print(\"Finish construciton\")\n",
    "    \n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "def split_dataset(data, test_ratio):\n",
    "    num_data = len(data)\n",
    "    print(num_data)\n",
    "    train_data = data[:int((1 - test_ratio) * num_data),:]\n",
    "    test_data = data[int((1 - test_ratio) * num_data):,:]\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(stock_data, 0.2)\n",
    "\n",
    "train_data = normalize(train_data)\n",
    "test_data = normalize(test_data)\n",
    "\n",
    "_X_train, _y_train = construct_dataset(train_data, seq_len)\n",
    "_X_test, _y_test = construct_dataset(test_data, seq_len)\n",
    "\n",
    "X_train = torch.tensor(_X_train).to(device, dtype)\n",
    "y_train = torch.tensor(_y_train).to(device, dtype)\n",
    "X_test = torch.tensor(_X_test).to(device, dtype)\n",
    "y_test = torch.tensor(_y_test).to(device, dtype)\n",
    "\n",
    "print(\"y_train: \", y_train.size())\n",
    "print(\"y_test: \", y_test.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make RNN model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a cell for recurrent neural network (RNN)\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(RNN, self).__init__()\n",
    "        # To initialize hidden vectors\n",
    "        self.hidden_size = H\n",
    "        \n",
    "        self.i2h = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.h2h = torch.nn.Linear(H, H, bias=True)\n",
    "        self.h2o = torch.nn.Linear(H, D_out, bias=True)\n",
    "        \n",
    "        self.activation_h = torch.nn.Tanh()\n",
    "        self.activation_o = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, i, h):\n",
    "        wx = self.i2h(i)\n",
    "        wh = self.h2h(h)\n",
    "        \n",
    "        h = wx + wh\n",
    "        h = self.activation_h(h)\n",
    "        \n",
    "        o = self.h2o(h)\n",
    "        o = self.activation_o(o)\n",
    "        return o, h\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size).to(dtype=dtype, device=device)\n",
    "    \n",
    "model = RNN(D_in, H, D_out).to(dtype=dtype, device=device)\n",
    "criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the model (Vanilla RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a recurrent neural netwok (RNN)\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, Layer):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(D_in, H, num_layers=Layer, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(H, D_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _status = self.rnn(x)\n",
    "        output = self.fc(output[:,-1])\n",
    "        return output\n",
    "    \n",
    "model = RNN(D_in, H, D_out, Layer).to(device, dtype)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model (Vanilla RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    y_train_pred = model(X_train)\n",
    "    \n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print(\"Epoch \" + str(epoch+1) + \"\\t Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test)\n",
    "    plt.figure()\n",
    "    plt.plot(_y_test)\n",
    "    plt.plot(y_test_pred.cpu().detach().numpy())\n",
    "    plt.legend(['original', 'prediction'])\n",
    "    plt.title(\"Stock prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the model (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a recurrent neural netwok (LSTM)\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out, Layer):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(D_in, H, num_layers=Layer, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(H, D_out, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _status = self.lstm(x)\n",
    "        x = self.fc(x[:,-1])\n",
    "        return x\n",
    "    \n",
    "model = LSTM(D_in, H, D_out, Layer).to(device, dtype)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    y_train_pred = model(X_train)\n",
    "    \n",
    "    loss = criterion(y_train_pred, y_train)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print(\"Epoch \" + str(epoch+1) + \"\\t Loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test)\n",
    "    plt.figure()\n",
    "    plt.plot(_y_test)\n",
    "    plt.plot(y_test_pred.cpu().detach().numpy())\n",
    "    plt.legend(['original', 'prediction'])\n",
    "    plt.title(\"Stock prediction\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
